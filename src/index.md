---
title: ESFSim Research Project
toc: True
---

# Abstract

Artificial intelligence (AI) is revolutionizing the way data is processed, allowing people to work more efficiently through improved data visualization and interpretation. One technology that builds upon data processing is digital twinning (DT), where replicas of real-world objects are modeled, constantly updated, and interactable through computers. The goal is to use the digital counterpart to improve the physical twin in a feedback loop, then introduce new data from the physical twin back into the digital twin as the former changes. Though interest in DT has been explosive in recent years, the integration of AI techniques into DT has yet to be thoroughly explored despite the natural synergies between the two technologies. This survey attempts to discuss and evaluate two primary approaches to integrating machine learning into digital twin ecosystems: i) building machine learning applications on top of digital twins and ii) using machine learning models as digital twins themselves. We present case studies from numerous domains, discovering that most integrations currently fall under the first approach, before identifying limitations in these existing approaches and areas for future development. Methods used in machine learning are also briefly mentioned.

# Introduction

The confluence of artificial intelligence (AI) and digital twinning (DT) is ushering in a new era of intelligent systems that mirror and work alongside the physical world. With the burgeoning interest and development in machine learning, especially deep learning, the way we process data has been revolutionized, enabling us to work more efficiently through improved data visualization, interpretation, and decision-making. Digital twinning takes this a step further by creating virtual replicas of real-world objects, constantly updating them, and making them interactive through computers. The goal is to use these digital counterparts to improve their physical twins in a closed feedback loop, where insights from the digital model are applied back to the physical system, leading to continuous improvement and optimization.

While digital twin development has seen explosive growth in recent years, the integration of AI techniques remains an area ripe for exploration, despite agreement that AI is a key enabling technology for digital twins [@fuller_digital_2020; @minerva_dt_2020]. The natural synergies between these two domains hold immense potential for transforming digital twin ecosystems, enabling us to build more dynamic, data-driven, and intelligent systems.

This paper identifies two primary approaches to integrating machine learning in the digital twins space:

- **Building machine learning applications on top of existing digital twins:** This approach utilizes AI to enhance specific aspects of a pre-existing digital twin, often focusing on data synthesis, processing, or decision-making support.

- **Using machine learning models as digital twins themselves:** This involves employing machine learning models as the core representation of the physical system, potentially eliminating the need for traditional, computationally expensive models and providing a more accessible option.

We provide an overview of key machine learning techniques, including classical algorithms, neural networks, physics-informed neural networks (PINNs), surrogate models, transfer learning, attention mechanisms, transformers, and foundation models. We then present case studies from various domains, exploring the current landscape of machine learning integration in digital twins, highlighting the limitations of existing approaches, and outlining promising areas for future development. Finally, we propose a framework for a generalizable, machine learning-based data-driven digital twin, followed by an evaluation of existing digital twin solutions from a [] perspective. % performance? whatever we end up doing with evaluation

# Digital Twins

Twinning was first introduced in the NASA Apollo program in the 1960s; after Apollo 13’s oxygen tank exploded and caused additional damage to the main engine, 15 simulators were set up to evaluate the extent of the damage and create a “twin” in virtual space. The “twin” enabled them to model the events leading up to the explosion and determine how to proceed [@allen_digital_2021]. The term “digital twin” itself would not emerge until 2002 when it was coined by Dr. Michael Grieves in the context of product lifecycle management (PLM). PLM is concerned with the data and processes used in tracking and handling a product as it progresses through the stages of its life, from development to decline [@segal_product_2023]. Grieves [@grieves_dt_2014] defines the digital twin as a virtual representation of a physical product. The digital counterpart allows for comparison between the designed specifications and the actual produced product, thereby creating a feedback loop that helps improve the manufacturing process. The digital twin comprises three main components: physical products in real space, virtual products in virtual space, and the connections between them, which consist of data and information that link the physical and virtual products, facilitating real-time monitoring and comparison.

Digital twins have been extrapolated to numerous domains since, beyond their original intended use in manufacturing [@fuller_digital_2020]; such domains include smart city development [@lv_solving_2021], agriculture [@allen_digital_2021; @purcell_agriculture_2023], and epidemiology and healthcare [@quilodran-casas_digital_2022; @moztarzadeh_metaverse_2023]. At the moment, there is no single authority or agreement on the term's definition, nor any standardized method of implementation. The concept is continuously evolving, even if slowly [@fuller_digital_2020], with new research and applications. Minerva et al. [@minerva_dt_2020] define a digital twin as a “comprehensive software representation of an individual [physical object]” or a set of such models that include all the “properties, conditions, and behavior(s)” of the physical object through data and modeling, all with the goal of tracking the physical object's life cycle. [@noauthor_digital_2019] offers a similar definition: the digital twin is a digital replica of a physical entity that utilizes sensor data to combine simulation and analytics, providing insights into both present and future operational states of the physical entity. The digital twin can be used alongside similar twins in an ensemble or used with analytical tools to optimize or diagnose performance under real-world conditions.

Within the scope of this paper, we define the digital twin as a system comprising the digital system and the physical twin or system. The digital twin is a complete virtual description of a physical system that attempts to “mirror” a physical system in its present or future state. The digital twin will integrate data from the real world as it becomes available. The physical twin describes the system as it exists in the real world, and it is what the digital twin is attempting to mirror. The system operates in a feedback loop, where insights from the real world are parsed through the digital twin and extracted, then applied back to the physical twin. As the digital twin is a model of the physical twin, we can explore models to represent the physical twin beyond tangible or visual representations, and into mathematical representations such as machine learning models.

# State of Machine Learning and Artificial Intelligence

Artificial intelligence (AI) has emerged as a transformative force in various domains, offering powerful techniques for data analysis, extracting patterns, and approximating complex, high-dimensional functions. We will focus on a subset of AI, machine learning, and we further broadly categorize the evolution of machine learning into two interconnected branches: classical algorithms and neural networks. We use “deep learning” and “neural networks” interchangeably. To fully establish the landscape in which digital twins are being developed and how machine learning is closely related to their development, it is also essential to understand how the two branches interact and describe the common and emergent methods currently employed.

## Machine Learning

Classical machine learning algorithms, with roots in statistical modeling and pattern recognition, witnessed significant advancements throughout the latter half of the 20th century. While techniques like linear regression and Naive Bayes can be traced back to the 19th century, this marked the period during which algorithms like logistic regression and early clustering methods emerged as powerful tools for data analysis across various fields. The continued development of support vector machines (SVMs) in the 1990s and the popularization of decision trees marked notable milestones, providing robust methods for classification and prediction tasks. The rise of big data and the demand for more sophisticated models handling complex relationships ultimately shifted the focus towards neural networks, but classic algorithms are still dominant in applications where interpretability and computational efficiency are paramount. They remain relevant and widely used, with innovative algorithms being proposed within the last decade.

There are a few key advantages to classic machine learning algorithms:

- The primary benefit is in interpretability, though there are exceptions. Explicit relationships are usually assumed between variables, which makes it easier to understand how input features influence the output. Additionally, there are clear decision boundaries; for example, trees and SVMs create visually interpretable decision boundaries in the feature space. You can trace how the algorithm divides data points into classes based on specific feature values.
- They often require fewer data points than deep learning methods to be effective and their performance on limited data is still comparable to neural networks.
- They require significantly less computational power to train and make inferences.

Subfields of machine learning, such as neural networks or deep learning, have recently enjoyed explosive popularity. However, the value of applying machine learning algorithms should still not be overlooked, and neural networks should not be treated as a panacea.

## Neural Networks

Neural networks were also introduced in the mid-1900s alongside the concept of artificial intelligence, though interest oscillated during the first few decades of research; the field frequently faced stagnation in the form of “AI winters,” periods of reduced interest and funding in AI, due to overoptimism and disappointing results [@noauthor_w_2005]. Some of these issues can be attributed to one massive bottleneck in implementation feasibility: computational power. Still, the initial architecture and concepts for many popular models used today began appearing in the 1980s and 1990s, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Around the 2000s, research into using graphics processing units (GPUs) for deep learning took off and reignited interest [@noauthor_not_2016], and we have since enjoyed countless breakthroughs in the last 15 years, from ImageNet to transformers. The development of neural networks will continue to heavily depend on hardware capabilities.

Traditional machine learning techniques, though powerful, may be limiting in some scenarios. They often could require knowledge of the function we are approximating beforehand, need more feature engineering, and can only represent certain types of functions. The key reason for using neural networks is that they can handle big data while approximating complex high-dimensional relationships and capturing important correlations. When trained correctly, they fit better to complex data and tasks in spaces that may not be linearly separable. There is also no need to identify and understand the most important feature ahead of time.

Neural networks also face some important drawbacks, in contrast to classic machine learning algorithms:
- One of the primary problems neural networks face is a lack of interpretability. Not only are the parameters rarely physically interpretable, but beyond the architecture and certain decisions made before running the model, we cannot be entirely certain as to how the neural network makes its decisions and returns an output. This is why the neural network is often called a ``black box.'' 
- Computational cost grows significantly, especially as the network gets deeper. This also depends on the data you are training and making inferences on. Training itself can also take weeks to complete.
- A neural network's extrapolation capacity is limited. If the model is not trained for a specific scenario, the model generally cannot make predictions about said scenario.
- The vanishing and exploding gradient problems: in the former, the activation function's gradients get progressively smaller through backpropagation, leading to weight updates becoming infinitesimal, if they happen at all. The latter is when updates to the model weights become excessively large due to accumulating large gradients in backpropagation. In both cases, model training is hindered somehow, e.g. prolonged, stopped altogether, or no longer learns from the training set.

## Attention
The purpose of attention in machine learning has been to serve as a way to quantify both the importance of tokens and relationship between different tokens within a given context window. With these characteristics of each token quantified, a machine learning model is able to better learn the relationships and patterns within the data without forgetting information from tokens in the context window. However, the calculation of these values for large context windows can be highly inefficient when performed sequentially such as in the case of a recurrent neural network(RNN).

## Transformers
In the case of large context windows, the adoption of the transformer model first introduced in 2017 ~\tocite has allowed for the ability to rapidly make these attention calculations by performing them simultaneously through the use of scaled matrix dot products. The reasoning behind this decision is that "dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code"~\tocite. Then once multiple of these attention calculations are performed they are then concatenated to produce multi-head attention that would allow for attending to all tokens within the context window. As a result, models are better able to learn relationships between tokens from any distance within the context window as well as their importance within the context window. This ability to be able to capture relationships over long ranges is part of the reason as to why the transformer architecture can be a possible architecture to be used for the implementation of our digital twin. The data that is collected by the various sensors is time-series data and is often cyclical in nature. However, this means that it is possible that more gradual changes in the data may go unnoticed as a result if the long range relationships in the data are not accounted for, which the transformer did so more effectively as compared to other architectures at the time the paper was published. However, the transformer does have some issues despite its ingenuity, one of which is the fact that the time complexity increases quadratically as the sequence length increases [Expand a more on the main transformer paper and then dive into other papers(Reformer,Longformer,etc.)]

## Foundation Model
A rapidly developing paradigm in how artificial intelligence systems are approached is heavily tied to foundation models. We define foundation models as general-purpose deep learning models that have been pretrained on diverse and massive datasets, which can then be fine-tuned to perform various downstream tasks. Essentially, they provide a "foundation" upon which more specialized models can be built. The models that would come to be known as foundation models had been in development for years, and the primary underlying concepts of foundation models---transfer learning and deep learning---have been researched for decades, but the term itself was recently coined and popularized by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI) in 2021 [@noauthor_introducing_center_foundation_2021]. The most well-known foundation models, at the moment, are large language models (LLMs) for use in the realm of natural language processing (NLP), such as the GPT, LLaMA, and BERT models. Multimodality in LLMs is also becoming increasingly popular. As a whole, foundation models' use cases extend far beyond language and into domains like climate, biology, and computer vision. Currently, based on existing foundation models, the standard for foundation model architecture is the transformer [@bommasani_foundation_2023, bommasani_reflections_2021].

Two defining characteristics of foundation models are transfer learning and scale [@Bommasani2021FoundationModels, thrun_lifelong_1998]. We have discussed transfer learning and what makes it a powerful technique. Specifically in the context of foundation models, we look at \textit{pre-training} and \textit{fine-tuning} as the primary means of transfer learning, wherein models are initially trained on broad datasets through ``surrogate tasks'' to capture relationships in the data. Then, these models are further trained for specific downstream tasks through fine-tuning, all while retaining the overall context. ``Scale,'' then, encompasses the general idea of growing to expand capability, and was enabled by three components: improved computer hardware, the transformer model architecture, and the increased availability of training data. Everything hinges on the availability of data and the ability to extract information from data.

As such, foundation models are currently especially desirable because of the flexibility and efficiency offered by extensive pre-training on such large volumes of data. They demonstrate improved reasoning ability in out-of-distribution tasks and are better at identifying relationships in data. These models are also somewhat adjacent to artificial general intelligence (AGI), the end goal for many current AI initiatives. These generalization capabilities are highly coveted in digital twin development, and benefits are further discussed in ~\ref{future_foundation}.

The above both promotes and has led to homogenization. This is beneficial in that improvements to models will be felt across all of its implementations, thereby promoting robustness. At the same time, this centralization is a major potential weakness, as the models' biases, errors, and risks may also be reflected across implementations. That is, the foundation model itself becomes a single point of failure [@Bommasani2021FoundationModels][@bommasani_reflections_2021]. Akin to a building, if the foundation is unstable, then everything built on top of the foundation will also be unstable. This concern is amplified by the fact that as with deep neural networks, foundation models are nigh uninterpretable; their inner workings are poorly understood [@Bommasani2021FoundationModels].

Challenges and limitations extend beyond code. Foundation models are time-consuming to train and compute-intensive, making their development inaccessible to most [@bommasani_reflections_2021]. Reproducibility is further hindered because datasets for pre-training major models are still largely unreleased to the public. This ultimately means much of the existing work is being done by corporations and development is not being shaped by academia or the larger machine learning communities. That many of the existing foundation models (e.g. BERT, GPT, LLaMA, and Gemini models) are widely available and generally affordable serves as a significant reason for the explosive interest among developer communities. Academia and these communities are also pressing corporations to be more transparent in divulging details related to their foundation models, such as source code, data sources, and labor [@bommasani_foundation_2023]. The ethics of foundation models, specifically in generative AI, from sourcing data to environmental effects, continue to be a point of contention.

## Fine-tuning
Recently models parameters have grown exponentially larger, doubling approximately every 6 months to take in greater variety and modality of input data, becoming foundation models. These large foundation models can leverage what it has learned onto downstream tasks given small amounts of additional training data. This has been pivotal to the explosion of fine-tuning methods. One inherent limitation of neural networks is that it's constrained to only the capabilities learned from the finite data it was trained on. Retraining the entire model is time-consuming and computationally expensive, so various techniques have been created to solve this issue. As model size grows, it becomes increasingly cheaper and faster to build atop the massive amounts of knowledge and skills of the existing foundation model by fine-tuning. The need for cheaply built AIs with knowledge of specific domains, has given rise to various fine-tuning methods used to train AI. Fine-tuning is the process of specializing the skills and knowledge of a pre-trained AI model on specific downstream tasks by modifying its weights or giving it access to the new data source instead of retraining it from scratch. It is currently being used to specialize foundation models for domain specific tasks targeted towards certain departments and fields [@jeong_fine-tuning_2024, gao_improving_2024, tinn_fine-tuning_2023, zheng_fine-tuning_2024]. However, fine-tuning tasks at scale remains costly, computationally expensive and time inefficient, constrained by hardware and budget limitations, and has a trade off between specialization and generalization, even when retraining a portion of the several billion weights, due to the original size of the parameters. Parallel distributed systems and high performance computing strategies have been created to solve this problem, making fine-tuning more accessible. In our AI driven DT we utilize fine-tuning to create a real-time connection between the digital and physical systems. The result is low costs as it side steps the need for training new models from scratch in order to incorporate old data, increases flexibility of large AI models to new tasks, and allows information from the physical system to be used by the AI in real time. 

## Parameter Efficient Fine-tuning Methods (PEFT)
Parameter Efficient Fine-Tuning (PEFT) adapts models to downstream tasks with minimal overhead and training time, by selectively modifying targeted parameters. PEFT has evolved into four main subcategories: additive, selective, reparameterized and hybrid fine-tuning. This organizational framework was first introduced by ``PEFT for Large Models: A Comprehensive Survey''{@han_parameter-efficient_2024}.

## Additive PEFT
Additive PEFT injects additional trainable parameters or modules in Transformer blocks, which do not modify the pre-trained weight layers. An example is adapters, which were first introduced by Parameter-Efficient Transfer Learning for NLP [@houlsby_parameter-efficient_2019]. This technique inserts adapter layers which provide extra trainable parameters for downstream tasks while keeping the pre-trained model parameters frozen, increasing the model depth for extraction of additional patterns used in specialized applications. Research on the efficiency of adapter based fine-tuning methods demonstrate that adapters have little benefit on NLU, are expensive to train and have higher deployment latency compared LoRA [@hu_lora_2021, randa_comparison_1999]. Much research has been done in improving the insertion location, efficiency and modification of adapters [@lei_conditional_2023, zhu_counter-interference_2021, he_towards_2022]. [did they improve to be competitive in any niches??]

## Reparameterized PEFT: LoRA
Reparameterized PEFT trains parameters with small size then recombines it with the pre-trained models during inference. It has evolved into Low-rank Decomposition (LoRA), a method of low-latency with downstream tasks taking almost the same amount of time as pre-trained model tasks due to their small size, compact memory as the A and B matrices in the paper are 10000 times tinier compared to typical fine-tuning on GPT-3, swapped on and off quickly when needed using a element-wise sum and trained quickly due to their small size [@hu_lora_2021]. The learnable parameter layer is split into matrices B and A representing the row and column vectors calculated from Singular Value Decomposition and sharing hyperparameter rank. This reduces redundant information stored inside excess parameters, reinforcing patterns and moving closer to the intrinsic dimension of the matrix [@aghajanyan_intrinsic_2020]. Less time is spent inserting layers since the position of \(\Delta W\) remains stationary in between the neural network. The process keeps pre-trained weights W frozen, and adds on several dense layers, which contain changes \(\Delta W\) made to the pre-trained weights. The additional matrix is decomposed into smaller matrices A and B, initially with A being set to randomly generated gaussian values and B set to 0. The size of the small matrices are determined by the user-defined rank hyperparamter, which works well for values in the low range of r=1-8, and starts falling off at 64. However, this is entirely dependent on the type and complexity of task that the model is trained on. This paper demonstrate LoRA is a competitive solution that performs as well as if not better than traditional fine-tuning, adaptor, or BitFit ect, on RoBERTa, DeBERTa, GPT-2 and GPT-3, E2E competition and the GLUE benchmark. The concept can also be applied to optimizing the pre-trained model's intrinsic dimension. Additional variations on the LoRA technique have been created in response to challenges related to selecting an appropriate rank for the problem at hand, over confidence and over fitting risks. 

## Reparameterized PEFT: QLoRA
Quantized Low-rank Decomposition (QLoRA) maintains full 16-bit fine-tuning performance while optimizing storage during training by leveraging the normal distribution of pre-trained model parameters to discretize inputs into block-wise segments [@dettmers_8-bit_2022] with ranges larger at the tails and smaller in the center of the bell curve. During quantization standard 16-bit floats are broken down into quantization constants and 4-bit Normal Floats (NF4) {@dettmers_case_2023}, and dequantized when needed for training on the CPU with minimal loss of accuracy {@nagel_white_2021}. Additionally, the quantization constants are quantized, further saving an additional 0.37 bits per parameter. When a large input is fed in to be trained, the optimization script is moved to the CPU to be temporarily stored as the input takes it space on the GPU to be trained. It is a form of paged memory training. QLoRA was used to fine-tune a 65B parameter model on a single 48GB GPU, a 16 fold improvement from 780 GB [@dettmers_qlora_2023]. 

## Retrieval Augmented Generation
RAG is a framework for keeping models updated with new information by connecting the pre-trained AI to an external authoritative knowledge source from which it can retrieve information when generating a response to enhance parameter knowledge without modification. This is especially useful in knowledge-intensive tasks, and first gained traction following the popularity of LLMs for public use. Unlike other gradient-based fine-tuning methods, it maintains a low-cost real-time connection between the DT with AI without the need for re-training any parameters in a gradient-free approach. RAG combines the instantaneous updates of information retrieval (IR), and the clarity of generation models. This provides flexibility in keeping the knowledge easily "expanded or modified, inspected or interpreted" {@lewis_retrieval-augmented_2021} by humans, allowing users to trace the provenance of decisions thereby making the process more transparent and trustworthy. However, tracing knowledge generation and continuous learning currently lacks satisfactory solutions. Additionally, RAG prevents the model from presenting false information if it does not have the answer and producing "hallucinations" {@marcus_next_2020}. Research also demonstrates efficacy in knowledge grounding by connecting a LLM to the real world {@fang_trace_2024}, personalization of AI to tailors to the users preferences or needs {@shi_eragent_2024}, and in-context learning where the model adapts to the specific context {@yuan_rag-driver_2024}. 

RAG was first coined in 2020 by {@lewis_retrieval-augmented_2021}, originating as a strategy for giving seq2seq models access to a vector index of Wikipedia. However it has roots in much older statistical techniques dating back to the 1970s to early question-answering systems {@simmons_natural_1970}. Naive RAG represents the basic methodology on which Advanced RAG and Modular RAG are built. It follows a "Retrieve-read" framework popular in knowledge graphs (KG). The pipeline begins with the cleaning and extraction of raw data into usable forms. This is encoded into a vector representation which is stored as indexes in a format efficient for retrieval. Once the user queries the LLM, their input is also turned into a vector representation. A semantic similarity algorithm calculates the numerical distance between the query and the items in the database, with the top K documents most similar to the query being retrieved. The selected documents are turned into context as part of the prompt and output as response.  

The semantic similarity is calculated by dividing the sentence into individual words and clauses, based on their semantic and syntactic structure. This comprises word similarity, sentence similarity and word order similarity. More precise algorithms also consider the synset, a synonym ring or group of data elements that are considered semantically equivalent for the purposes of information retrieval, and analyze subtle shades of meaning to create a more fine-grained definition. Techniques of focusing solely on the nouns and verbs during this calculation have been used with a tradeoff in language understanding and accuracy. The pipeline involves tokenizing the the sentences into individual word vectors for calculations. Their meanings are quantified by cross referencing it with words in a lexical database where similarities are found based on a tree-like structure containing the distance, depth of the subsummer root node {@pawar_calculating_2018}. Alternative methods include calculating similarities by referencing a linguistic corpus, word co-occurence methods which are common in information retrieval (IR) {@luo_neural_2022}, where lists of meaningful words are mapped to queries and similarity score is calculated based off of context. There has also been research in finding the similarity based on search engine results, such as the Google Similarity Distance {@cilibrasi_google_2007}. These words then reassembled into phrases, and then sentences whose similarities can also be calculated.